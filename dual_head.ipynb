{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129d893e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# ----------------------------\n",
    "# Seed\n",
    "# ----------------------------\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# ----------------------------\n",
    "# Dataset\n",
    "# ----------------------------\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, texts, y_quality, y_relevance, tokenizer, max_len=128, meta=None):\n",
    "        self.texts = texts\n",
    "        self.yq = np.asarray(y_quality, dtype=np.int64)\n",
    "        self.yr = np.asarray(y_relevance, dtype=np.int64)\n",
    "        self.tok = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.meta = meta\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        enc = self.tok(\n",
    "            self.texts[i], truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors=\"pt\"\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "        item[\"yq\"] = torch.tensor(int(self.yq[i]))\n",
    "        item[\"yr\"] = torch.tensor(int(self.yr[i]))\n",
    "        if self.meta is not None:\n",
    "            item[\"meta\"] = torch.tensor(self.meta[i], dtype=torch.float32)\n",
    "        return item\n",
    "\n",
    "# ----------------------------\n",
    "# Mean Pooler\n",
    "# ----------------------------\n",
    "class MeanPooler(nn.Module):\n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        mask = attention_mask.unsqueeze(-1).float()\n",
    "        summed = (last_hidden_state * mask).sum(dim=1)\n",
    "        denom = mask.sum(dim=1).clamp(min=1e-9)\n",
    "        return summed / denom\n",
    "\n",
    "# ----------------------------\n",
    "# Dual-head Model\n",
    "# ----------------------------\n",
    "class ReviewMTL(nn.Module):\n",
    "    def __init__(self, encoder_name, n_quality=3, n_relevance=2, proj_dim=256, dropout=0.1, meta_dim=0):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(encoder_name)\n",
    "        self.pool = MeanPooler()\n",
    "        H = self.encoder.config.hidden_size\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(H, H),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(H, proj_dim)\n",
    "        )\n",
    "        self.meta_bn = nn.BatchNorm1d(meta_dim) if meta_dim > 0 else None\n",
    "        head_in = H + (meta_dim if meta_dim > 0 else 0)\n",
    "        self.head_quality = nn.Sequential(nn.Dropout(dropout), nn.Linear(head_in, n_quality))\n",
    "        self.head_relev = nn.Sequential(nn.Dropout(dropout), nn.Linear(head_in, n_relevance))\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, meta=None):\n",
    "        out = self.encoder(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "        emb = self.pool(out.last_hidden_state, attention_mask)\n",
    "        z = nn.functional.normalize(self.proj(emb), p=2, dim=-1)\n",
    "        if meta is not None and meta.numel() > 0:\n",
    "            if self.meta_bn is not None:\n",
    "                meta = self.meta_bn(meta)\n",
    "            emb = torch.cat([emb, meta], dim=1)\n",
    "        q_logits = self.head_quality(emb)\n",
    "        r_logits = self.head_relev(emb)\n",
    "        return z, q_logits, r_logits\n",
    "\n",
    "# ----------------------------\n",
    "# Losses\n",
    "# ----------------------------\n",
    "def supcon_loss(z, y, temperature=0.07):\n",
    "    B = z.size(0)\n",
    "    sim = (z @ z.t()) / temperature\n",
    "    eye = torch.eye(B, device=z.device, dtype=torch.bool)\n",
    "    sim.masked_fill_(eye, -1e9)\n",
    "    pos = (y.unsqueeze(0) == y.unsqueeze(1)) & (~eye)\n",
    "    log_denom = torch.logsumexp(sim, dim=1, keepdim=True)\n",
    "    log_prob = sim - log_denom\n",
    "    pos_counts = pos.sum(dim=1).clamp(min=1)\n",
    "    loss_per = -(log_prob * pos).sum(dim=1) / pos_counts\n",
    "    return loss_per.mean()\n",
    "\n",
    "def ce_smooth_weighted(logits, targets, n_classes, smoothing=0.0, class_weight=None):\n",
    "    if smoothing <= 0.0 and class_weight is None:\n",
    "        return nn.functional.cross_entropy(logits, targets)\n",
    "    with torch.no_grad():\n",
    "        true = torch.zeros_like(logits)\n",
    "        true.fill_(smoothing / (n_classes - 1))\n",
    "        true.scatter_(1, targets.unsqueeze(1), 1.0 - smoothing)\n",
    "    logp = nn.functional.log_softmax(logits, dim=-1)\n",
    "    loss_per = -(true * logp).sum(dim=1)\n",
    "    if class_weight is not None:\n",
    "        w = class_weight[targets]\n",
    "        loss_per = loss_per * w\n",
    "        return loss_per.sum() / w.sum().clamp(min=1e-9)\n",
    "    return loss_per.mean()\n",
    "\n",
    "# ----------------------------\n",
    "# Training\n",
    "# ----------------------------\n",
    "def train_model(train_ds, val_ds, n_quality=3, n_relevance=2, cfg=None):\n",
    "    set_seed(42)\n",
    "    meta_dim = train_ds.meta.shape[1] if train_ds.meta is not None else 0\n",
    "    tok = AutoTokenizer.from_pretrained(cfg.encoder_name, use_fast=True)\n",
    "    model = ReviewMTL(cfg.encoder_name, n_quality=n_quality, n_relevance=n_relevance, meta_dim=meta_dim).to(cfg.device)\n",
    "    train_dl = DataLoader(train_ds, batch_size=cfg.classes_per_batch*cfg.samples_per_class, shuffle=True)\n",
    "    val_dl = DataLoader(val_ds, batch_size=cfg.classes_per_batch*cfg.samples_per_class, shuffle=False)\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr_heads)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(cfg.device==\"cuda\"))\n",
    "\n",
    "    for ep in range(cfg.epochs_joint):\n",
    "        model.train()\n",
    "        for b in train_dl:\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            with torch.autocast(device_type=(\"cuda\" if cfg.device==\"cuda\" else \"cpu\"), dtype=torch.float16, enabled=(cfg.device==\"cuda\")):\n",
    "                meta = b.get(\"meta\")\n",
    "                meta = meta.to(cfg.device) if isinstance(meta, torch.Tensor) else None\n",
    "                z, q_logits, r_logits = model(b[\"input_ids\"].to(cfg.device), b[\"attention_mask\"].to(cfg.device), meta)\n",
    "                Jq = ce_smooth_weighted(q_logits, b[\"yq\"].to(cfg.device), n_classes=n_quality, smoothing=cfg.label_smoothing)\n",
    "                Jr = ce_smooth_weighted(r_logits, b[\"yr\"].to(cfg.device), n_classes=n_relevance)\n",
    "                Jc = supcon_loss(z, b[\"yq\"].to(cfg.device), temperature=cfg.temperature)\n",
    "                loss = Jq + Jr + cfg.lambda_contrastive * Jc\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "    return model, tok\n",
    "\n",
    "# ----------------------------\n",
    "# Prediction / Evaluation\n",
    "# ----------------------------\n",
    "@torch.no_grad()\n",
    "def predict(model, dataset, cfg):\n",
    "    model.eval()\n",
    "    all_q, all_r = [], []\n",
    "    dl = DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "    for b in dl:\n",
    "        meta = b.get(\"meta\")\n",
    "        meta = meta.to(cfg.device) if isinstance(meta, torch.Tensor) else None\n",
    "        _, q_logits, r_logits = model(b[\"input_ids\"].to(cfg.device), b[\"attention_mask\"].to(cfg.device), meta)\n",
    "        all_q.append(q_logits.softmax(-1).cpu().numpy())\n",
    "        all_r.append(r_logits.softmax(-1).cpu().numpy())\n",
    "    return np.vstack(all_q), np.vstack(all_r)\n",
    "\n",
    "def evaluate_model_classification_report(model, dataset, cfg):\n",
    "    yq_true = dataset.yq\n",
    "    yr_true = dataset.yr\n",
    "    q_pred, r_pred = predict(model, dataset, cfg)\n",
    "    yq_pred = q_pred.argmax(axis=1)\n",
    "    yr_pred = r_pred.argmax(axis=1)\n",
    "    print(\"Quality Classification Report\")\n",
    "    print(classification_report(yq_true, yq_pred, digits=4))\n",
    "    print(\"Relevance Classification Report\")\n",
    "    print(classification_report(yr_true, yr_pred, digits=4))\n",
    "\n",
    "# ----------------------------\n",
    "# Train/Test Split\n",
    "# ----------------------------\n",
    "# X_texts = list of your text data\n",
    "# y_quality = list/array of quality labels\n",
    "# y_relevance = list/array of relevance labels\n",
    "X_texts   = df['reviewText'].tolist()      # list of review strings\n",
    "y_quality = df['qualityLevel'].values            # numpy array of quality labels\n",
    "y_relevance = df['isRelevant'].values        # numpy array of relevance labels\n",
    "X_train, X_val, yq_train, yq_val, yr_train, yr_val = train_test_split(\n",
    "    X_texts, y_quality, y_relevance, test_size=0.2, random_state=42, stratify=y_quality\n",
    ")\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(cfg.encoder_name, use_fast=True)\n",
    "train_ds = ReviewDataset(X_train, yq_train, yr_train, tok, max_len=cfg.max_len)\n",
    "val_ds = ReviewDataset(X_val, yq_val, yr_val, tok, max_len=cfg.max_len)\n",
    "\n",
    "# ----------------------------\n",
    "# Train and Evaluate\n",
    "# ----------------------------\n",
    "model, tok = train_model(train_ds, val_ds, n_quality=3, n_relevance=2, cfg=cfg)\n",
    "evaluate_model_classification_report(model, val_ds, cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f812cab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model_classification_report(model, dl, cfg):\n",
    "    model.eval()\n",
    "    all_q_preds, all_r_preds = [], []\n",
    "    all_q_true, all_r_true = [], []\n",
    "\n",
    "    for b in dl:\n",
    "        meta = b.get(\"meta\")\n",
    "        meta = meta.to(cfg.device) if isinstance(meta, torch.Tensor) else None\n",
    "        _, q_logits, r_logits = model(\n",
    "            b[\"input_ids\"].to(cfg.device),\n",
    "            b[\"attention_mask\"].to(cfg.device),\n",
    "            meta\n",
    "        )\n",
    "        q_preds = q_logits.argmax(-1).cpu().numpy()\n",
    "        r_preds = r_logits.argmax(-1).cpu().numpy()\n",
    "        all_q_preds.append(q_preds)\n",
    "        all_r_preds.append(r_preds)\n",
    "        all_q_true.append(b[\"yq\"].numpy())\n",
    "        all_r_true.append(b[\"yr\"].numpy())\n",
    "\n",
    "    all_q_preds = np.concatenate(all_q_preds)\n",
    "    all_r_preds = np.concatenate(all_r_preds)\n",
    "    all_q_true = np.concatenate(all_q_true)\n",
    "    all_r_true = np.concatenate(all_r_true)\n",
    "\n",
    "    print(\"=== Quality Task ===\")\n",
    "    print(classification_report(all_q_true, all_q_preds, digits=4))\n",
    "\n",
    "    print(\"=== Relevance Task ===\")\n",
    "    print(classification_report(all_r_true, all_r_preds, digits=4))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
